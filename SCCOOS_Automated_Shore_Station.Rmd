---
title: "SCCOOS_Automated_Shore_Stations"
author: "Megan Hepner"
date: "7/20/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#rm(list=ls()) 

library(cowplot) #get_legend 
library(dygraphs) #dygraphs 
library(ggplot2) #ggplot
library(tidyverse)
library(vegan) #spec, diversity
library(htmlwidgets)
library(xts)
library(lubridate) 
library(zoo) #as.yearmon
library(shiny)

```

## SCCOOS Automated Shore Station Data 

```{r cars}

ass_temp = read_csv("2018_07_20_automated_shore_stations.csv") #A tibble: 86,384 x 3

#Parsed with column specification:
#cols(station = col_character(),time = col_datetime(format = ""),temperature = col_double())

ass_temp_date = ass_temp %>%  
  na.omit() %>% 
  mutate(date = as.Date(time, format ="%m/%d/%Y/%HH/%mm/%ss"), "%Y-%m") %>%
  mutate(year = format(as.Date(time, format="%m/%d/%Y/%HH/%mm/%ss"), "%Y")) %>% 
  mutate(month = format(as.Date(time, format="%m/%d/%Y/%HH/%mm/%ss"), "%m")) %>% 
  mutate(year = as.integer(year)) %>%
  mutate(month = as.integer(month)) %>%
  mutate(year_month = make_datetime(year, month)) 
  
ass_temp_year_month = ass_temp_date %>%
  group_by(year_month, station) %>% 
    summarize(
      temperature_mean = mean(temperature),
      temperature_n = length(temperature),
      temperature_sd = sd(temperature),
      temperature_se = temperature_sd /sqrt(temperature_n)) 
  
ass_temp_year = ass_temp_date %>% 
  group_by(year, station) %>% 
    summarize(
      temperature_mean = mean(temperature),
      temperature_n = length(temperature),
      temperature_sd = sd(temperature),
      temperature_se = temperature_sd /sqrt(temperature_n)) 

#ass_temp_date$time = ymd(ass_temp_date$time)  
```

##  Plots

```{r pressure, echo=FALSE}

ass_temp_plot = ggplot(ass_temp_date, aes(x=year_month, y=temperature_mean, color=station, group=station))+
  geom_line(aes(group=station), lwd = 1)+
  geom_point()+
  geom_errorbar(aes(ymax=temperature_mean+temperature_se,ymin=temperature_mean-temperature_se),width=0.1)+
  labs(title= "", x="Year", y="Mean Temperature (± SE)")+
  #scale_color_manual(values =  c("blue","red"),breaks = c("1", "0"), labels=c("Protected","Not Protected"))+
  #scale_x_continuous(limits = c(2005, 2018), breaks = c(1999,2000,2001,2002, 2003,2004,2005, 2006, 2007,2008, 2009,2010,2011, 2012,2013, 2014,2015, 2016), labels=c("","2000","","2002","","2004","","2006","","2008","","2010","","2012","","2014","","2016"))+
  theme_bw()+
  theme(
    legend.position="none",
    panel.grid.major=element_blank(),
    panel.grid.minor=element_blank(),
    axis.title.x=element_text(size=12),
    axis.title.y=element_text(size=12),
    title=element_text(size=12))

ass_temp_plot_year = ggplot(ass_temp_year, aes(x=year, y=temperature_mean, color=station, group=station))+
  geom_line(aes(group=station), lwd = 1)+
  geom_point()+
  geom_errorbar(aes(ymax=temperature_mean+temperature_se,ymin=temperature_mean-temperature_se),width=0.1)+
  labs(title= "", x="Year", y="Mean Temperature (± SE)")+
  #scale_color_manual(values =  c("blue","red"),breaks = c("1", "0"), labels=c("Protected","Not Protected"))+
  #scale_x_continuous(limits = c(2005, 2018), breaks = c(1999,2000,2001,2002, 2003,2004,2005, 2006, 2007,2008, 2009,2010,2011, 2012,2013, 2014,2015, 2016), labels=c("","2000","","2002","","2004","","2006","","2008","","2010","","2012","","2014","","2016"))+
  theme_bw()+
  theme(
    legend.position="none",
    panel.grid.major=element_blank(),
    panel.grid.minor=element_blank(),
    axis.title.x=element_text(size=12),
    axis.title.y=element_text(size=12),
    title=element_text(size=12))

```

## CDPH Data

California Department of Public Health weekly observations for Pseudo-nitzschia and Alexandrium. 

Density Codes:
R = Rare (1)
P = Present (2)
C = Common (3)
A = Abundant (4)

The RAI is based on the sampling effort (total tow length), percent composition, and settled cell mass (in milliliters).

```{r July 2018 CDPH data Pseudo nitzschia}

CDPH_2018 = list.files(path="CDPH_Data/Pseudo_nitzschia/", pattern = "*.csv")
#"2018_06_week_3_Pn.csv" "2018_06_week_4_Pn.csv" "2018_07_week_1_Pn.csv" "2018_07_week_2_Pn.csv" "2018_07_week_3_Pn.csv"

Pn_2018_07_wk1 = read_csv("CDPH_Data/Pseudo_nitzschia/2018_07_week_1_Pn.csv") #36x11 
Pn_2018_07_wk2 = read_csv("CDPH_Data/Pseudo_nitzschia/2018_07_week_2_Pn.csv") #38x11
Pn_2018_07_wk3 = read_csv("CDPH_Data/Pseudo_nitzschia/2018_07_week_3_Pn.csv") #32x11
Pn_2018_07_wk4 = read_csv("CDPH_Data/Pseudo_nitzschia/2018_07_week_4_Pn.csv") #32x11

CDPH_2018_07 = dplyr::bind_rows(Pn_2018_07_wk1, Pn_2018_07_wk2) #74x11
CDPH_2018_07 = dplyr::bind_rows(CDPH_2018_07, Pn_2018_07_wk3) #106x11
CDPH_2018_07 = dplyr::bind_rows(CDPH_2018_07, Pn_2018_07_wk4) #138x11

CDPH_Pn_2018_07 = CDPH_2018_07 %>%
  select(`Date -Sampled-`,`Sample Site`,`Percent Composition`,`Density -code-`,`RA Index`,Latitude,Longitude)

# Parsed with column specification:
#   `ID #` = col_character(),
#   `Date -Sampled-` = col_character(),
#   `NtoS -code-` = col_integer(),
#   `Sample Site` = col_character(),
#   `Species -code-` = col_character(),
#   `Percent Composition` = col_double(),
#   `Density -code-` = col_character(),
#   RA_Rank = col_integer(),
#   `RA Index` = col_double(),
#   Latitude = col_double(),
#   Longitude = col_double()

write_csv(CDPH_Pn_2018_07, "CDPH_Data/Pseudo_nitzschia/CDPH_Pn_2018_07.csv")

CDPH_Pn_2018_07_max = CDPH_Pn_2018_07 %>%
  group_by(Sample_Site,Latitude,Longitude) %>%
  summarize(
    percent_composition_max = max(Percent_Composition))%>%
  #arrange(Sample_Site) %>%
  arrange(percent_composition_max)

write_csv(CDPH_Pn_2018_07_max, "CDPH_Data/Pseudo_nitzschia/CDPH_Pn_2018_07_max.csv")

```

```{r July 2018 CDPH data Alexandria}

###ALEXANDRIUM 

CDPH_2018 = list.files(path="CDPH_Data/Alexandria/", pattern = "*.csv")
#"2018_06_week_3_Al.csv" "2018_06_week_4_Al.csv" "2018_07_week_1_Al.csv" "2018_07_week_2_Al.csv" "2018_07_week_3_Al.csv"

#Al_2018_06_wk4 = read_csv("CDPH_Data/Alexandria/2018_06_week_4_Al.csv")
Al_2018_07_wk1 = read_csv("CDPH_Data/Alexandria/2018_07_week_1_Al.csv") #26x11 
Al_2018_07_wk2 = read_csv("CDPH_Data/Alexandria/2018_07_week_2_Al.csv") #22x11
Al_2018_07_wk3 = read_csv("CDPH_Data/Alexandria/2018_07_week_3_Al.csv") #24x11
Al_2018_07_wk4 = read_csv("CDPH_Data/Alexandria/2018_07_week_4_Al.csv") #18x11

CDPH_Al_2018_07 = dplyr::bind_rows(Al_2018_07_wk1, Al_2018_07_wk2) #48x11 
CDPH_Al_2018_07 = dplyr::bind_rows(CDPH_Al_2018_07, Al_2018_07_wk3) #72x11
CDPH_Al_2018_07 = dplyr::bind_rows(CDPH_Al_2018_07, Al_2018_07_wk4) #90x11 

CDPH_Al_2018_07 = CDPH_Al_2018_07 %>%
  select(`Date -Sampled-`,`Sample Site`,`Percent Composition`,`Density -code-`,`RA Index`,Latitude,Longitude)

write_csv(CDPH_Al_2018_07, "CDPH_Data/Alexandria/CDPH_Al_2018_07.csv")

CDPH_Al_2018_07_max = CDPH_Al_2018_07 %>%
  group_by(Sample_Site,Latitude,Longitude) %>%
  summarize(
    percent_composition_max = max(Percent_Composition))%>%
  #arrange(Sample_Site) %>%
  arrange(percent_composition_max) #56x4 

write_csv(CDPH_Al_2018_07_max, "CDPH_Data/Alexandria/CDPH_Al_2018_07_max.csv")

```

```{r August 2018 CDPH data}

Pn_2018_08_wk1 = read_csv("CDPH_Data/Pseudo_nitzschia/2018_08_week_1_Pn.csv") #29x11 
Pn_2018_08_wk2 = read_csv("CDPH_Data/Pseudo_nitzschia/2018_08_week_2_Pn.csv") #
Pn_2018_08_wk3 = read_csv("CDPH_Data/Pseudo_nitzschia/2018_08_week_3_Pn.csv") #
Pn_2018_08_wk4 = read_csv("CDPH_Data/Pseudo_nitzschia/2018_08_week_4_Pn.csv") #

CDPH_2018_08 = dplyr::bind_rows(Pn_2018_08_wk1, Pn_2018_08_wk2) #
CDPH_2018_08 = dplyr::bind_rows(CDPH_2018_08, Pn_2018_08_wk3) #
CDPH_2018_08 = dplyr::bind_rows(CDPH_2018_08, Pn_2018_08_wk4) #

CDPH_Pn_2018_08 = CDPH_2018_08 %>%
  select(`Date -Sampled-`,`Sample Site`,`Percent Composition`,`Density -code-`,`RA Index`,Latitude,Longitude)

write_csv(CDPH_Pn_2018_08, "CDPH_Data/Pseudo_nitzschia/CDPH_Pn_2018_08_wk_1-4.csv")

```